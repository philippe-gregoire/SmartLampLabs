{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use a Jupyter Notebook in Watson Studio\n",
    "* Each executable cell of the notebook is preceded with a `In [ ]` indicator.   \n",
    "* When the bracket is empty, it means that the cell has not been executed.\n",
    "* To execute a cell, either click the Run button above, or hit Shift-Enter key combination\n",
    "* The bracket turns to `In [*]` while the code is running, and then a sequence number once completed.\n",
    "* The output of a code cell will be displayed below that cell, either in text or graphical format\n",
    "* Cells can be executed in any sequence, but the code dependencies are usually requiring a sequential execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize some of the libraries we will need\n",
    "A set of python packages are present in the Jupyter notebook environment, and need to be made known to the current execution namespace:\n",
    "* Python standard libs\n",
    "* `numpy` for numerical/mathematical functions in python\n",
    "* `pandas` is the python analytics for data library, providing table-like objects known as Panda DataFrames\n",
    "* `matplotlib` is a library for plotting mathematical functions\n",
    "* `bokeh` is another graphical library for interactive plotting and graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,types\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh.plotting as bok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cloud Object Storage (COS) API to access data\n",
    "This API is documented at https://console.bluemix.net/docs/services/cloud-object-storage/libraries/python.html#python   \n",
    "\n",
    "**First pass instructions:**\n",
    "Here we will access files of historical data points that have been collected and stored by a Watson IoT to COS Stream similar to the one you have been building earlier in this hands-on session.\n",
    "This earlier run has collected data over about 5 days, showing the daily cycles in sensors readings.\n",
    "\n",
    "In the first pass of the lab, we will access historical data, in the second pass, you will access your own data, which is currently being collected by the WIOTP->COS data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second pass instructions:**\n",
    "Once you will have reached the bottom of this notebook execution, we will re-run the notebook on your own dataset, and run a few extra steps to generate a concatenated data file for Machine Learning ingestion.\n",
    "\n",
    "So, we will need to insert you own COS credentials belwo, instead of the ones for the historical data:\n",
    "* Select the code cell below, and comment out the whole cell contents (add a `#` in front of each line, you can use the Ctrl-/ key shortcut for this)\n",
    "* Position the cursor after the commented section\n",
    "* On the top right side of the notebook, select the 1001 icon, which opens the Files/Connections drawer\n",
    "* Freom the `Connections` tab, locate the connection to your COS, which should be named with a `cloud-object-storage-` prefix\n",
    "* From the connection, select `Insert to code`\n",
    "* This will insert a new `credentials_n = {}` code block.\n",
    "* **Make sure** that the variable is suffixed with `1`, i.e. `credentials_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials for accessing the historical data stored in COS\n",
    "# @hidden_cell\n",
    "credentials_1 = {\n",
    "  'iam_url':'https://iam.ng.bluemix.net/oidc/token',\n",
    "  'api_key':'wwnkueE80Sv8jYLAHl7YLi9WIdz9dEUL3Ca5H6NN4JAz',\n",
    "  'resource_instance_id':'crn:v1:bluemix:public:cloud-object-storage:global:a/7f9dc5344476457f2c0f53244a1825db:ea11a568-b1e1-4743-85a4-ba297aa46370::',\n",
    "  'url':'https://s3-api.us-geo.objectstorage.service.networklayer.com'\n",
    "}\n",
    "# @hidden_cell\n",
    "# credentials_1 = {\n",
    "#   'iam_url':'https://iam.ng.bluemix.net/oidc/token',\n",
    "#   'api_key':'rZDp4xWnLHvEWLrk1J_FBRJyY3g4vtt6CXWa59yvNlhu',\n",
    "#   'resource_instance_id':'crn:v1:bluemix:public:cloud-object-storage:global:a/6edb55179ec85cbb11b70fed0e12c861:15f5b9b1-9cf9-4d16-99a5-7c408282997c::',\n",
    "#   'url':'https://s3-api.us-geo.objectstorage.service.networklayer.com'\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First pass:** We have configured the stream flow to store in the `raspilamp` bucket, with a `raspiLamp1/lampdata_[0-9]*_[0-9]*.csv` file path pattern, i.e. CSV files with a timestamp suffix\n",
    "\n",
    "**Second pass:** change the variables below to reflect the structure of COS file path storage you have setup in the flow:\n",
    "``` python\n",
    "bucketName='raspilamp-20180420-x' # where x is your lamp number\n",
    "dataFileRegExp='lampdata_[0-9]*_[0-9]*.csv'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data used for the Raspi Lamp 1 instance\n",
    "bucketName='raspilamp'\n",
    "#bucketName='raspilamp-20180420-x'\n",
    "\n",
    "dataFileRegExp='raspiLamp1/lampdata_[0-9]*_[0-9]*.csv'\n",
    "#dataFileRegExp='lampdata_[0-9]*_[0-9]*.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generic code cell will use the credentials filled-in the cell above in the `credentials_1` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "#cos = ibm_boto3.client(service_name='s3',\n",
    "cos = ibm_boto3.client('s3',\n",
    "    ibm_api_key_id=credentials_1['api_key'],\n",
    "    ibm_auth_endpoint=credentials_1['iam_url'],\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=credentials_1['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now list the files from the given bucket whose names match the regular expression defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFilePrefix='' if len(dataFileRegExp.split('/'))==1 else dataFileRegExp.split('/')[0]\n",
    "bucketFiles=cos.list_objects(Bucket=bucketName,Prefix=dataFilePrefix,MaxKeys=1000000)\n",
    "keys = [bucketFile['Key'] for bucketFile in bucketFiles['Contents'] if re.match(dataFileRegExp,bucketFile['Key'])]\n",
    "print(\"There are {0} lampdata files out of {1} files that match regexp in bucket {2}\".format(len(keys),len(bucketFiles['Contents']),bucketName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually read the files from the bucket.   \n",
    "This may be a lengthy operation depending on how many files have been collected.   \n",
    "The `In [*]:` in front of the cell will turn to a sequence number once completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about your possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n",
    "objs=[cos.get_object(Bucket=bucketName, Key=key) for key in keys]\n",
    "print(\"Loaded {0} lampdata files\".format(len(objs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We collect streaming bodies objects for all files, and add a __iter__ method to allow file-like operations\n",
    "strbodies=[obj['Body'] for obj in objs if obj['Body']]\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "for strb in strbodies: strb.__iter__= types.MethodType(__iter__,strb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data files into pandas and concatenate into a single dataframe\n",
    "Now that we have adressed the files from COS storage, we need to read them into objects that can be used by Data Science libraries.   \n",
    "Here we use the Panda library.   \n",
    "\n",
    "The following cell loads and convert the CSV files contents into Panda DataFrames, which are then concatenated to form a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from CSV files input, dropping duplicates\n",
    "dfs = [pd.read_csv(strb).drop_duplicates() for strb in strbodies]\n",
    "df=pd.concat(dfs)\n",
    "\n",
    "#print(\"Loaded lamp data rows count:\\n{0}\\n{1}\".format(df.count(),df.dtypes))\n",
    "# Show types and non-N/A counts\n",
    "pd.DataFrame({'types':df.dtypes,'count':df.count()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data for empty values\n",
    "The following code cell drops lines which have empty cells (although in the stored sampling there should be none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with N/A cells\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust the data frame columns types\n",
    "* The panda CSV read has not necessarily identified the column types accurately.   \n",
    "* In the following code cell, we force some columns to numeric format, and we add a `date` column built from the absolute timestamp column `ts` that was added by the streams flow.\n",
    "* Some of the timestamp fields are known to be large integers, but have been converted to floats, we also convert tem back to `int64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data types, somehow the integer columns are not seen as numeric\n",
    "df[['ldr','leds','rawTemp','solar']] = df[['ldr','leds','rawTemp','solar']].apply(pd.to_numeric)\n",
    "\n",
    "# Add a data column from ts column\n",
    "df['date'] = pd.to_datetime(df['ts'],unit='s')\n",
    "\n",
    "# Convert back to int64\n",
    "df['ts']=df['ts'].astype(np.int64)\n",
    "df['dts']=df['dts'].astype(np.int64)\n",
    "df['rawTemp']=df['rawTemp'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the index for the timestamp date\n",
    "We are operating on timeseries, so we will now use the absolute `date` as an index. In order to simplify further handling, we duplicate the `date` column and use the `dtidx` copy as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index on date\n",
    "df['dtidx']=df['date']\n",
    "df=df.set_index('dtidx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display modified DataFrame's types\n",
    "This is to show the final typology of the dataframe after transtyping and cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'types':df.dtypes,'count':df.count()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the IoT data\n",
    "We are now ready to start analysing the IoT data loaded from COS storage.\n",
    "\n",
    "Panda dataframes have many built-in methods to display statistic, let's start by getting a glimpse of the data by printing the first and last 5 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display 5 first and last lines\n",
    "pd.concat([df.head(5),df.tail(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `describe()` function to print statistics on numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate vizualisations for various aspects of the data\n",
    "The first tools used to explore a dataset will be some graphical representations.\n",
    "\n",
    "Using the `plot()` primitive, it is possible to quickly graph several attributes of a DataFrame\n",
    "\n",
    "We'll start by plotting timeseries graphs of the sensors values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual graphs of sensors\n",
    "p_sens=df.plot(kind='line',x='date',y=['ldr','solar','humidity','pressure','temp','temperature','rawTemp'],figsize=(20,15),subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore potential relationships of sensors\n",
    "We are now interested in finding out which sensors have correlated values.   \n",
    "The graphs plotted above seem to show that `ldr` and `solar` are strongly correlated, but that other values are not much related to each others, we'll verify this through a correlation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data frame with sensor readings only\n",
    "df_sens=df[['ldr','solar','humidity','pressure','temp','temperature','rawTemp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate correlation matrix\n",
    "The correlation matrix will yield a first-level evaluation on the correlations of attributes with each others.\n",
    "We will compute the matrix and display it.   \n",
    "The correlation is a value normalized on [-1,+1] which indicates how much parameters are co-related. the higher the value, the stronger the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute correlation matrix\n",
    "df_corr=df_sens.corr()\n",
    "df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display of correlation matrix\n",
    "To make things more understandable than an array of numbers, we can plot the matrix's heatmap.   \n",
    "A red color indicates a high correlation, while blue is uncorrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix as heatmap, Red is high correlation, yellow is medium, blue is low\n",
    "axcorr=plt.matshow(df_corr,cmap=plt.cm.rainbow).axes\n",
    "axcorr.set_xticklabels([' ']+df_corr.columns.tolist())\n",
    "axcorr.set_yticklabels([' ']+df_corr.columns.tolist())\n",
    "for label in axcorr.xaxis.get_ticklabels(): label.set_rotation(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focus on LDR vs solar relationship\n",
    "Now that this early investigations have proven that there is a strong relationship between the solar and LDR values, we'll now focus on finding more about this.  \n",
    "We'll start by plotting the two values on the same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First plot the two data lines superimposed on the same graph\n",
    "plight=df.plot(kind='line',x='date',y=['ldr','solar'],figsize=(20,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the values are not within the same range, we'll normalize them in the [0-1] range and plot again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now graph ldr and solar along time, after normalizing the data to the 0-1 range\n",
    "dfSolLDR=df[['solar','ldr']]\n",
    "dfSolLDRNorm=(dfSolLDR - dfSolLDR.min())/(dfSolLDR.max() - dfSolLDR.min())\n",
    "dfSolLDRNorm['date']=df['date']\n",
    "pSolLDR=dfSolLDRNorm.plot(kind='line',x='date',y=['ldr','solar'],figsize=(20,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display dependency relationship of ldr and solar values\n",
    "The normalized plot above shows that the `solar` and `ldr` values track each others, but do not exactly follow the same pattern all the time. We will analyze this further through a scatter plot.\n",
    "\n",
    "For this we will use a scatter graph of `solar` vs `ldr`.   \n",
    "In order to study the influence of temperature, we vary the intensity of the dots color based on the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show relationship between ldr and solar, color by temperature\n",
    "p=df.plot(kind='scatter',x='solar',y='ldr',c='temperature',figsize=(20,10),colormap='Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot scatter matrix of all sensors\n",
    "To confirm sensors relationships, we can also get a full cross-map of sensors relation ship displayed graphically.   \n",
    "This will be a bit lengthy, but yields exhaustive results that show how sensors relate to each others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter matrix of sensors, display kde on the diagonal \n",
    "from pandas.plotting import scatter_matrix\n",
    "pscat=scatter_matrix(df_sens, alpha=0.2, figsize=(20,20), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Interactive graphs plotting with `bokeh`\n",
    "`bokeh` is another plotting library, which adds interactive capabilities to what matplotlib can do.   \n",
    "Being interactive, it will work better on smaller datasets, so we will first aggregate data by the minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreggate by minutes. We use a grouper on the date, and retain the average value on the window\n",
    "# This generates a new DataFrame with by-minute averages\n",
    "dfMin=df.groupby([pd.Grouper(key='date',freq='min')]).mean()\n",
    "dfMin.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the bokeh graph with tools enabled. You will be able to interact with the graph using the tools palette on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-out the axis from the by-minute data frame \n",
    "xSolar = dfMin['solar']\n",
    "yLdr = dfMin['ldr']\n",
    "\n",
    "# Compute min and max temperatrures\n",
    "tMin=dfMin['temperature'].min()\n",
    "tMax=dfMin['temperature'].max()\n",
    "print(\"temperature range: {0} - {1}\".format(tMin,tMax))\n",
    "\n",
    "# Generate an array of colors hex RGB values, where the intensity of the green color is proportional to temperature, and red when t is NaN\n",
    "c = [ \"#%02x%02x00\" % (0xff if np.isnan(t) else 0x00, 0x00 if np.isnan(t) else int(0xff*(tMax-t)/(tMax-tMin))) for t in dfMin['temperature']]\n",
    "\n",
    "## We could generate varying size dot through a radius array in the same fashion as for colors\n",
    "#radii = np.random.random(size=N) * 1.5\n",
    "#colors = [\n",
    "#    \"#%02x%02x%02x\" % (int(r), int(g), 150) for r, g in zip(50+2*x, 30+2*y)\n",
    "#]\n",
    "\n",
    "# Enable some of the bokeh interactive tools\n",
    "#ALL_TOOLS=\"hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,tap,save,box_select,poly_select,lasso_select,\"\n",
    "TOOLS=\"hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,reset,save,box_select,poly_select,lasso_select\"\n",
    "p = bok.figure(tools=TOOLS)\n",
    "\n",
    "#p.scatter(x, y, radius=radii,fill_color=colors, fill_alpha=0.6,line_color=None)\n",
    "p.scatter(xSolar, yLdr,fill_alpha=0.6,color=c,name=\"scatter\")\n",
    "\n",
    "# Instruct bokeh to output to the notebook\n",
    "bok.output_notebook()\n",
    "\n",
    "bok.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A glimpse at ML techniques: Classification and Clustering\n",
    "\n",
    "Clustering is a Machine Learning technique whereby algorithms are applied to data sets to attempt to group data points together.   \n",
    "This is often refered to as one of the 'Unsupervised Machine Learning' techniques.\n",
    "\n",
    "In this lab section, we will show how to apply clustering to our lamp's [solar, ldr] cartesian points.\n",
    "\n",
    "The following code is based on the SciKit Learn ![](http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png)   \n",
    "Machine Learning library, see http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\n",
    "\n",
    "To show the impact of the choice of a Clustering algorithm on the effectiveness of classification, we will add our RaspiLamp dataset to the sample provided by SciKit-Learn.\n",
    "The sample code runs 9 algorithms and charts them together. This allows to have a feeling for the effectiveness of a given algorithm versus the shape and structure of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapted from http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Lamp Data: Reduce volume of data to analyze through aggregation\n",
    "We will reduce the number of data points for our lamps to run through the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a row of graph for our lamp data, create n_samples aggregates of averaged values\n",
    "dfSubMeans=df[['date','solar','ldr']]\n",
    "dfSubMeans=df.groupby(pd.cut(dfSubMeans['date'],n_samples)).mean()\n",
    "\n",
    "# Convert to a numPy array of arrays for the solar and ldr columns, add a dummy zeroed column\n",
    "npArrLamp=(dfSubMeans.as_matrix(columns=['solar','ldr']),np.zeros(n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all datasets into an array to iterate over\n",
    "datasets = [\n",
    "    (npArrLamp,{'eps': .15, 'n_neighbors': 2}),\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "# Set up plot details\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "# Default model parameters\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3}\n",
    "\n",
    "# iterate over datssets and algorithms to plot graphs\n",
    "plot_num = 1\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(np.int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "\n",
    "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                                             '#f781bf', '#a65628', '#984ea3',\n",
    "                                             '#999999', '#e41a1c', '#dede00']),\n",
    "                                      int(max(y_pred) + 1))))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to interpret the output\n",
    "You can see on the graphs that depending on the structure of data, some algorithms work better than others\n",
    "\n",
    "For our dataset, it seems that DBSCAN is the best option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********\n",
    "# This is the end of the first pass.\n",
    "For the second pass run on your own data, we will need to reset the notebook:\n",
    "* From the `Kernel` menu at the top, select `Restart & Clear Output`\n",
    "* All cells results should be blanked, and all cells execution status back to `In [ ]:`\n",
    "* Go back to the top of the notebook and re-run it following second-pass instructions\n",
    "*********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is to be executed only for the second pass on your own data\n",
    "\n",
    "## Machine Learning setup\n",
    "In this section, we will just write back the full concatenated data frame to Object Storage, so that it can be fed to the Machine Learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide on the name of the file object in COS\n",
    "allSensorFileName='lampdata_All.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a subset of the columns to CSV format\n",
    "csvStr=df.to_csv(columns=['ts','solar','ldr','humidity','pressure','temperature'],encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the CSV data to a file object in COS\n",
    "import io\n",
    "cos.upload_fileobj(io.BytesIO(csvStr.encode('utf-8')),bucketName,allSensorFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Verify the dataset by loading it back into a new dataframe\n",
    "allObjs=cos.get_object(Bucket=bucketName, Key=allSensorFileName) \n",
    "allStrbod=allObjs['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "allStrbod.__iter__= types.MethodType(__iter__,allStrbod)\n",
    "\n",
    "dfAllSens = pd.read_csv(allStrbod)\n",
    "dfAllSens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make timestamp to a date and set as index\n",
    "dfAllSens['date'] = pd.to_datetime(dfAllSens['ts'],unit='s')\n",
    "dfAllSens['dtidx']=dfAllSens['date']\n",
    "dfAllSens=dfAllSens.set_index('dtidx')\n",
    "\n",
    "dfAllSens.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: run DBSCAN clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the two light-related sensors\n",
    "dfLightSens=dfAllSens[['solar','ldr']].copy()\n",
    "\n",
    "# Access sklearn clustering\n",
    "import sklearn.cluster\n",
    "\n",
    "# Select of the of the clustering algorithms\n",
    "#skClust = sklearn.cluster.KMeans(n_clusters=3)\n",
    "#skClust = sklearn.cluster.SpectralClustering(n_clusters=3)\n",
    "skClust = sklearn.cluster.DBSCAN()\n",
    "\n",
    "# run the algo on the data\n",
    "run=skClust.fit(dfLightSens.as_matrix(),{'eps': .15, 'n_neighbors': 2})\n",
    "\n",
    "# Add clustering result column to light sensors dataframe\n",
    "dfLightSens['cluster']=skClust.labels_\n",
    "\n",
    "dfLightSens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display relationship between ldr and solar, colored by cluster\n",
    "pLightSens=dfLightSens.plot(kind='scatter',x='solar',y='ldr',c='cluster',figsize=(20,10),colormap='hsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
